{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Clipping\n",
    "Certain types of deep neural networks, especially, simple ones without any other type regularization and a relatively large number of layers, can suffer from exploding gradient problems. The exploding gradient problem is a scenario where large loss gradients accumulate during backpropagation, which will eventually result in very large weight updates during training. As a consequence, the updates will be very unstable and fluctuate a lot, which often causes severe problems during training. This is also a particular problem for unbounded activation functions such as ReLU.\n",
    "\n",
    "One common, classic technique for avoiding exploding gradient problems is the so-called gradient clipping approach. Here, we simply set gradient values above or below a certain threshold to a user-specified min or max value. In PyTorch, there are several ways for performing gradient clipping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Basic Clipping\n",
    "\n",
    "The simplest approach to gradient clipping in PyTorch is by using the torch.nn.utils.clip_grad_value_ function. For example, if we have instantiated a PyTorch model from a model class based on torch.nn.Module (as usual), we can add the following line of code in order to clip the gradients to [-1, 1] range:\n",
    "However, notice that via this approach, we can only specify a single clip value, which will be used for both the upper and lower bound such that gradients will be clipped to the range [-clip_value, clip_value].\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.nn.utils.clip_grad_value_(parameters=model.parameters(), clip_value=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Custom Lower and Upper Bounds\n",
    "\n",
    "If we want to clip the gradients to an unsymmetric interval around zero, say [-0.1, 1.0], we can take a different approach by defining a backwards hook:\n",
    "This backward hook only needs to be defined once after instantiating the model. Then, each time after calling the backward method, it will clip the gradients before running the model.step() method."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for param in model.parameters():\n",
    "    param.register_hook(lambda gradient: torch.clamp(gradient, -0.1, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Norm-clipping\n",
    "\n",
    "Lastly, there's a third clipping option, torch.nn.utils.clip_grad_norm_, which clips the gradients using a vector norm as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.utils.clip_grad_norm_(model.parameters(), 1., norm_type=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
