{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "learning_rate = 0.1\n",
    "num_epochs = 10\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([128, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([128])\n",
      "79\n"
     ]
    }
   ],
   "source": [
    "for image , label in train_loader:\n",
    "    print('Image batch dimensions:' , image.shape)\n",
    "    print('Image label dimensions:' , label.shape)\n",
    "    break\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ConvNet,self).__init__()\n",
    "        \n",
    "        # calculate same padding:\n",
    "        # (w - k + 2*p)/s + 1 = o\n",
    "        # => p = (s(o-1) - w + k)/2\n",
    "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3,3), stride=(1,1), padding=1)\n",
    "        self.pool_1 = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2), padding=0)\n",
    "        self.conv_2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3,3), stride=(1,1), padding=1)\n",
    "        self.pool_2 = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2), padding=0)\n",
    "        self.linear_1 = nn.Linear(7*7*16, num_classes)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.conv_1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool_1(out)\n",
    "        \n",
    "        out = self.conv_2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.pool_2(out)\n",
    "        \n",
    "        out = self.linear_1(out.view(-1,7*7*16))\n",
    "        pro = F.softmax(out, dim=1)\n",
    "        return pro\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = ConvNet(10)\n",
    "\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_acc(model, data_loader):\n",
    "    model.eval()\n",
    "    correct_pred, num_examples = 0,0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in data_loader:\n",
    "            features = features.to(device)\n",
    "            targets = targets.to(device)\n",
    "            pro = model(features)\n",
    "            _, pre = torch.max(pro, 1)\n",
    "            num_examples += targets.size(0)\n",
    "            correct_pred += (pre==targets).sum()\n",
    "        return correct_pred.float()/num_examples *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/010 | Batch 000/469 | Loss: 2.3040\n",
      "Epoch 001/010 | Batch 050/469 | Loss: 2.3002\n",
      "Epoch 001/010 | Batch 100/469 | Loss: 2.2966\n",
      "Epoch 001/010 | Batch 150/469 | Loss: 2.2811\n",
      "Epoch 001/010 | Batch 200/469 | Loss: 2.1769\n",
      "Epoch 001/010 | Batch 250/469 | Loss: 1.8251\n",
      "Epoch 001/010 | Batch 300/469 | Loss: 1.8491\n",
      "Epoch 001/010 | Batch 350/469 | Loss: 1.7781\n",
      "Epoch 001/010 | Batch 400/469 | Loss: 1.7344\n",
      "Epoch 001/010 | Batch 450/469 | Loss: 1.8122\n",
      "Epoch: 001/010 training accuracy: 71.04%\n",
      "Time elapsed: 0.13 min\n",
      "Epoch 002/010 | Batch 000/469 | Loss: 1.8571\n",
      "Epoch 002/010 | Batch 050/469 | Loss: 1.7402\n",
      "Epoch 002/010 | Batch 100/469 | Loss: 1.7255\n",
      "Epoch 002/010 | Batch 150/469 | Loss: 1.7450\n",
      "Epoch 002/010 | Batch 200/469 | Loss: 1.6342\n",
      "Epoch 002/010 | Batch 250/469 | Loss: 1.7062\n",
      "Epoch 002/010 | Batch 300/469 | Loss: 1.6978\n",
      "Epoch 002/010 | Batch 350/469 | Loss: 1.6770\n",
      "Epoch 002/010 | Batch 400/469 | Loss: 1.6821\n",
      "Epoch 002/010 | Batch 450/469 | Loss: 1.7381\n",
      "Epoch: 002/010 training accuracy: 75.89%\n",
      "Time elapsed: 0.26 min\n",
      "Epoch 003/010 | Batch 000/469 | Loss: 1.6905\n",
      "Epoch 003/010 | Batch 050/469 | Loss: 1.6998\n",
      "Epoch 003/010 | Batch 100/469 | Loss: 1.7057\n",
      "Epoch 003/010 | Batch 150/469 | Loss: 1.7089\n",
      "Epoch 003/010 | Batch 200/469 | Loss: 1.7589\n",
      "Epoch 003/010 | Batch 250/469 | Loss: 1.7346\n",
      "Epoch 003/010 | Batch 300/469 | Loss: 1.7210\n",
      "Epoch 003/010 | Batch 350/469 | Loss: 1.6922\n",
      "Epoch 003/010 | Batch 400/469 | Loss: 1.7123\n",
      "Epoch 003/010 | Batch 450/469 | Loss: 1.7651\n",
      "Epoch: 003/010 training accuracy: 76.13%\n",
      "Time elapsed: 0.40 min\n",
      "Epoch 004/010 | Batch 000/469 | Loss: 1.6959\n",
      "Epoch 004/010 | Batch 050/469 | Loss: 1.7136\n",
      "Epoch 004/010 | Batch 100/469 | Loss: 1.7503\n",
      "Epoch 004/010 | Batch 150/469 | Loss: 1.6822\n",
      "Epoch 004/010 | Batch 200/469 | Loss: 1.6617\n",
      "Epoch 004/010 | Batch 250/469 | Loss: 1.6816\n",
      "Epoch 004/010 | Batch 300/469 | Loss: 1.6872\n",
      "Epoch 004/010 | Batch 350/469 | Loss: 1.7307\n",
      "Epoch 004/010 | Batch 400/469 | Loss: 1.6601\n",
      "Epoch 004/010 | Batch 450/469 | Loss: 1.6783\n",
      "Epoch: 004/010 training accuracy: 77.30%\n",
      "Time elapsed: 0.54 min\n",
      "Epoch 005/010 | Batch 000/469 | Loss: 1.7078\n",
      "Epoch 005/010 | Batch 050/469 | Loss: 1.6557\n",
      "Epoch 005/010 | Batch 100/469 | Loss: 1.7589\n",
      "Epoch 005/010 | Batch 150/469 | Loss: 1.7286\n",
      "Epoch 005/010 | Batch 200/469 | Loss: 1.6422\n",
      "Epoch 005/010 | Batch 250/469 | Loss: 1.6612\n",
      "Epoch 005/010 | Batch 300/469 | Loss: 1.7051\n",
      "Epoch 005/010 | Batch 350/469 | Loss: 1.7183\n",
      "Epoch 005/010 | Batch 400/469 | Loss: 1.6783\n",
      "Epoch 005/010 | Batch 450/469 | Loss: 1.7120\n",
      "Epoch: 005/010 training accuracy: 77.78%\n",
      "Time elapsed: 0.68 min\n",
      "Epoch 006/010 | Batch 000/469 | Loss: 1.7198\n",
      "Epoch 006/010 | Batch 050/469 | Loss: 1.7222\n",
      "Epoch 006/010 | Batch 100/469 | Loss: 1.7481\n",
      "Epoch 006/010 | Batch 150/469 | Loss: 1.6844\n",
      "Epoch 006/010 | Batch 200/469 | Loss: 1.6967\n",
      "Epoch 006/010 | Batch 250/469 | Loss: 1.6973\n",
      "Epoch 006/010 | Batch 300/469 | Loss: 1.6029\n",
      "Epoch 006/010 | Batch 350/469 | Loss: 1.6984\n",
      "Epoch 006/010 | Batch 400/469 | Loss: 1.7215\n",
      "Epoch 006/010 | Batch 450/469 | Loss: 1.6678\n",
      "Epoch: 006/010 training accuracy: 77.59%\n",
      "Time elapsed: 0.83 min\n",
      "Epoch 007/010 | Batch 000/469 | Loss: 1.6861\n",
      "Epoch 007/010 | Batch 050/469 | Loss: 1.7527\n",
      "Epoch 007/010 | Batch 100/469 | Loss: 1.6128\n",
      "Epoch 007/010 | Batch 150/469 | Loss: 1.6068\n",
      "Epoch 007/010 | Batch 200/469 | Loss: 1.6615\n",
      "Epoch 007/010 | Batch 250/469 | Loss: 1.6255\n",
      "Epoch 007/010 | Batch 300/469 | Loss: 1.6825\n",
      "Epoch 007/010 | Batch 350/469 | Loss: 1.7323\n",
      "Epoch 007/010 | Batch 400/469 | Loss: 1.6923\n",
      "Epoch 007/010 | Batch 450/469 | Loss: 1.6630\n",
      "Epoch: 007/010 training accuracy: 78.16%\n",
      "Time elapsed: 0.98 min\n",
      "Epoch 008/010 | Batch 000/469 | Loss: 1.6893\n",
      "Epoch 008/010 | Batch 050/469 | Loss: 1.6459\n",
      "Epoch 008/010 | Batch 100/469 | Loss: 1.6667\n",
      "Epoch 008/010 | Batch 150/469 | Loss: 1.7184\n",
      "Epoch 008/010 | Batch 200/469 | Loss: 1.6428\n",
      "Epoch 008/010 | Batch 250/469 | Loss: 1.6908\n",
      "Epoch 008/010 | Batch 300/469 | Loss: 1.7367\n",
      "Epoch 008/010 | Batch 350/469 | Loss: 1.6702\n",
      "Epoch 008/010 | Batch 400/469 | Loss: 1.6810\n",
      "Epoch 008/010 | Batch 450/469 | Loss: 1.6897\n",
      "Epoch: 008/010 training accuracy: 78.51%\n",
      "Time elapsed: 1.13 min\n",
      "Epoch 009/010 | Batch 000/469 | Loss: 1.6467\n",
      "Epoch 009/010 | Batch 050/469 | Loss: 1.7179\n",
      "Epoch 009/010 | Batch 100/469 | Loss: 1.7121\n",
      "Epoch 009/010 | Batch 150/469 | Loss: 1.6952\n",
      "Epoch 009/010 | Batch 200/469 | Loss: 1.7211\n",
      "Epoch 009/010 | Batch 250/469 | Loss: 1.7142\n",
      "Epoch 009/010 | Batch 300/469 | Loss: 1.7570\n",
      "Epoch 009/010 | Batch 350/469 | Loss: 1.6826\n",
      "Epoch 009/010 | Batch 400/469 | Loss: 1.6629\n",
      "Epoch 009/010 | Batch 450/469 | Loss: 1.6389\n",
      "Epoch: 009/010 training accuracy: 78.72%\n",
      "Time elapsed: 1.27 min\n",
      "Epoch 010/010 | Batch 000/469 | Loss: 1.6103\n",
      "Epoch 010/010 | Batch 050/469 | Loss: 1.6808\n",
      "Epoch 010/010 | Batch 100/469 | Loss: 1.6013\n",
      "Epoch 010/010 | Batch 150/469 | Loss: 1.6661\n",
      "Epoch 010/010 | Batch 200/469 | Loss: 1.6302\n",
      "Epoch 010/010 | Batch 250/469 | Loss: 1.7364\n",
      "Epoch 010/010 | Batch 300/469 | Loss: 1.6491\n",
      "Epoch 010/010 | Batch 350/469 | Loss: 1.7222\n",
      "Epoch 010/010 | Batch 400/469 | Loss: 1.6621\n",
      "Epoch 010/010 | Batch 450/469 | Loss: 1.6525\n",
      "Epoch: 010/010 training accuracy: 78.99%\n",
      "Time elapsed: 1.41 min\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_ind, (features, targets) in enumerate(train_loader):\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        pro = model(features)\n",
    "        loss = F.cross_entropy(pro, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if not batch_ind%50:\n",
    "            print('Epoch %03d/%03d | Batch %03d/%03d | Loss: %.4f' % ((epoch+1), num_epochs , batch_ind, \n",
    "                  len(train_loader), loss))\n",
    "    with torch.no_grad():\n",
    "        print('Epoch: %03d/%03d training accuracy: %.2f%%' % (\n",
    "              epoch+1, num_epochs, \n",
    "              metric_acc(model, train_loader)))\n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 78.97%\n"
     ]
    }
   ],
   "source": [
    "print('Test accuracy: %.2f%%' % (metric_acc(model, test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
