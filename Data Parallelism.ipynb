{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Implementation Details\n",
    "#To use DataParallel, in the \"Model\" section (i.e., the corresponding code cell) we replace\n",
    "\n",
    "model.to(device)\n",
    "model = VGG16(num_features=num_features, num_classes=num_classes)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "    model = nn.DataParallel(model)\n",
    "    \n",
    "    \n",
    "#and let the DataParallel class take care of the rest. Note that in order for this to work, the data currently needs to be on the first cuda device, \"cuda:0\". Otherwise, we will get a RuntimeError: all tensors must be on devices[0]. Hence, we define device below, which we use to transfer the input data to during training. Hence, make sure you set\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "#and not\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "#(or any other CUDA device number), so that in the training loop, we can use\n",
    "\n",
    "for i, (features, targets) in enumerate(data_loader):\n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "                \n",
    "#If you look at the implementation part\n",
    "\n",
    "#### DATA PARALLEL START ####\n",
    "\n",
    "model = VGG16(num_features=num_features, num_classes=num_classes)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "#### DATA PARALLEL END ####\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "#### DATA PARALLEL START ####\n",
    "cost_fn = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "you notice that the CrossEntropyLoss (we could also use the one implemented in nn.functional) is not part of the model. Hence, the loss will be computed on the device where the target labels are, which is the default device (usually the first GPU). This is the reason why the outputs are gathered on the first/default GPU. I sketched a more detailed outline of the whole process below:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
